{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#반복문 버전 ( 단일변수 활용)\n",
    "#### 2) 모멘텀이 있는 역전파 학습 수행하기\n",
    "\n",
    "\n",
    "#함수 선택부\n",
    "\n",
    "set_lr_decay_func(fixed_decay)  #학습률 조정 함수 선택\n",
    "set_activ_func(logi_func) #활성화 함수 선택\n",
    "set_grad_func(logi_func_deriv)  #활성화 함수 도함수 선택\n",
    "set_cost_func(sse_cost) #비용함수 선택\n",
    "set_weight_init_func(xavier_initialization) #가중치 초기화 함수 선택\n",
    "\n",
    "# 가중치 초기화\n",
    "weights = []  #가중치 저장용 리스트\n",
    "for(i) in range(len(layer_sizes)-1):\n",
    "    weights.append(call_weight_init_func(layer_sizes[i]+1, layer_sizes[i+1]))\n",
    "# 학습률 및 모멘텀 변수 초기화\n",
    "\n",
    "initial_lr = 0.004 # 초기 학습률\n",
    "decay_rate = 0.0001 # 학습률 감소율\n",
    "momentum = 0.84 # 모멘텀\n",
    "\n",
    "# 모멘텀을 위한 가중치 업데이트 변수 초기화\n",
    "v_w = []\n",
    "for(i) in range(len(weights)):\n",
    "    v_w.append(np.zeros_like(weights[i]))\n",
    "# 배치 크기 설정\n",
    "batch_size = 64  # 또는 당신의 데이터셋에 적합한 다른 크기\n",
    "num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "\n",
    "# 종료 조건 설정\n",
    "n_epochs =1000 \n",
    "gradient_threshold = 1e-8\n",
    "target_cost = 650\n",
    "\n",
    "J_history = [J]\n",
    "print(\"weights are initialized by \" + cur_weight_init_func.__name__)\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # 학습률 동적 조정\n",
    "    alpha = call_lr_decay_func(initial_lr, epoch, decay_rate, n_epochs)\n",
    "\n",
    "    # 데이터를 무작위로 섞기\n",
    "    indices = np.arange(X.shape[0])  # X의 길이만큼 인덱스 배열 생성\n",
    "    np.random.shuffle(indices)       # 인덱스 배열을 무작위로 섞음\n",
    "    X_shuffled = X_arr[indices]          # 섞인 인덱스에 따라 X를 섞음\n",
    "    y_shuffled = y_arr[indices]          # 섞인 인덱스에 따라 y를 섞음\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # 배치 데이터 가져오기\n",
    "        start = batch * batch_size\n",
    "        end = min(start + batch_size, X_arr.shape[0])\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # 편향 항 추가\n",
    "        y_onehot_batch = one_hot_encode(y_batch, 10)\n",
    "\n",
    "        # 비용의 순전파\n",
    "        a3, a2, a1, a0, in3, in2, in1 = front_propag(weights[0], weights[1], weights[2], X_batch, y_batch)\n",
    "\n",
    "        # 오차의 역전파\n",
    "        back_propag(a3, a2, a1, a0, in3, in2, in1, X_batch, y_onehot_batch)\n",
    "\n",
    "        grad23, grad12, grad01 = back_propag(a3, a2, a1, a0,in3, in2, in1, X_batch, y_onehot_batch)\n",
    "        grads = [grad01, grad12, grad23]\n",
    "\n",
    "        # # # 모멘텀을 계산\n",
    "        for i in range(len(weights)):\n",
    "            v_w[i] = momentum * v_w[i] + alpha * grads[i]\n",
    "        \n",
    "        # 가중치 업데이트\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] -= v_w[i]\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      # 에포크의 끝에서 손실 계산 및 출력\n",
    "      #J = total_cost_func(w01, w12,w23, X, y)\n",
    "      J = total_cost_func(weights[0], weights[1],weights[2], X, y)\n",
    "      print(f\"Epoch {epoch}: Cost = {J} LR = {alpha}\")\n",
    "        \n",
    "      # 그래디언트 크기가 임계값 이하면 학습 종료\n",
    "      if np.linalg.norm(grad01) + np.linalg.norm(grad12) + np.linalg.norm(grad23) < gradient_threshold:\n",
    "          print(f\"Stopped at epoch {epoch}: Gradient norm below threshold.\")\n",
    "          break\n",
    "      # 오차가 target_cost 미만이면 학습 종료 (거의 최적)\n",
    "    \n",
    "      if J < target_cost:\n",
    "          print(f\"Stopped at epoch {epoch}: Cost below {target_cost}.\")\n",
    "          break\n",
    "    #J_history.append(J)\n",
    "\n",
    "print(f\"Epoch {epoch + 1}: Cost = {J} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 비 반복문 버전 \n",
    "\n",
    "#### 2) 모멘텀이 있는 역전파 학습 수행하기\n",
    "\n",
    "# # 랜덤한 실수로 가중치를 초기화\n",
    "# w01 = np.random.rand(w01_size[0], w01_size[1])\n",
    "# w12 = np.random.rand(w12_size[0], w12_size[1])\n",
    "# w23 = np.random.rand(w23_size[0], w23_size[1])\n",
    "# print(\"w01 and w12 are all initialized with random values in [0, 1].\")\n",
    "\n",
    "#불러오기\n",
    "\n",
    "#w01, w12,w23 = load_weights(\"my_model_weights.npz\")\n",
    "\n",
    "#함수 선택부\n",
    "\n",
    "set_lr_decay_func(fixed_decay)  #학습률 조정 함수 선택\n",
    "set_activ_func(logi_func) #활성화 함수 선택\n",
    "set_grad_func(logi_func_deriv)  #활성화 함수 도함수 선택\n",
    "set_cost_func(sse_cost) #비용함수 선택\n",
    "set_weight_init_func(xavier_initialization) #가중치 초기화 함수 선택\n",
    "\n",
    "# 가중치 초기화\n",
    "print (w01_size, w12_size, w23_size)\n",
    "w01 = call_weight_init_func(w01_size[0], w01_size[1])\n",
    "w12 = call_weight_init_func(w12_size[0], w12_size[1])\n",
    "w23 = call_weight_init_func(w23_size[0], w23_size[1])\n",
    "\n",
    "# 학습률 및 모멘텀 변수 초기화\n",
    "\n",
    "initial_lr = 0.004 # 초기 학습률\n",
    "decay_rate = 0.0001 # 학습률 감소율\n",
    "momentum = 0.84 # 모멘텀\n",
    "\n",
    "print(\"vectorized implementation\")\n",
    "v_w01 = np.zeros_like(w01)\n",
    "v_w12 = np.zeros_like(w12)\n",
    "v_w23 = np.zeros_like(w23)\n",
    "\n",
    "# 배치 크기 설정\n",
    "batch_size = 64  # 또는 당신의 데이터셋에 적합한 다른 크기\n",
    "num_batches = int(np.ceil(X.shape[0] / batch_size))\n",
    "\n",
    "# 종료 조건 설정\n",
    "n_epochs = 5000\n",
    "gradient_threshold = 1e-8\n",
    "target_cost = 650\n",
    "\n",
    "J_history = [J]\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # 학습률 동적 조정\n",
    "    alpha = call_lr_decay_func(initial_lr, epoch, decay_rate, n_epochs)\n",
    "\n",
    "    # 데이터를 무작위로 섞기\n",
    "    indices = np.arange(X.shape[0])  # X의 길이만큼 인덱스 배열 생성\n",
    "    np.random.shuffle(indices)       # 인덱스 배열을 무작위로 섞음\n",
    "    X_shuffled = X_arr[indices]          # 섞인 인덱스에 따라 X를 섞음\n",
    "    y_shuffled = y_arr[indices]          # 섞인 인덱스에 따라 y를 섞음\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # 배치 데이터 가져오기\n",
    "        start = batch * batch_size\n",
    "        end = min(start + batch_size, X_arr.shape[0])\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # 편향 항 추가\n",
    "        y_onehot_batch = one_hot_encode(y_batch, 10)\n",
    "\n",
    "        # 비용의 순전파\n",
    "        a3, a2, a1, a0, in3, in2, in1 = front_propag(w01, w12, w23, X_batch, y_batch)\n",
    "\n",
    "        # 오차의 역전파\n",
    "        grad23, grad12, grad01 = back_propag(a3, a2, a1, a0,in3, in2, in1, X_batch, y_onehot_batch)\n",
    "\n",
    "        # 모멘텀을 계산\n",
    "        v_w23 = momentum * v_w23 + alpha * grad23\n",
    "        v_w12 = momentum * v_w12 + alpha * grad12\n",
    "        v_w01 = momentum * v_w01 + alpha * grad01\n",
    "\n",
    "        # 가중치 업데이트\n",
    "        w23 -= v_w23\n",
    "        w12 -= v_w12\n",
    "        w01 -= v_w01\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      # 에포크의 끝에서 손실 계산 및 출력\n",
    "      J = total_cost_func(w01, w12,w23, X, y)\n",
    "      print(f\"Epoch {epoch}: Cost = {J} LR = {alpha}\")\n",
    "\n",
    "        \n",
    "      # 그래디언트 크기가 임계값 이하면 학습 종료\n",
    "      if np.linalg.norm(grad01) + np.linalg.norm(grad12) + np.linalg.norm(grad23) < gradient_threshold:\n",
    "          print(f\"Stopped at epoch {epoch}: Gradient norm below threshold.\")\n",
    "          break\n",
    "\n",
    "      # 오차가 target_cost 미만이면 학습 종료 (거의 최적)\n",
    "    \n",
    "      if J < target_cost:\n",
    "          print(f\"Stopped at epoch {epoch}: Cost below {target_cost}.\")\n",
    "          break\n",
    "      # J_history.append(J)\n",
    "\n",
    "print(f\"Epoch {epoch + 1}: Cost = {J} \")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
